\documentclass[letterpaper]{article}
\usepackage{aaai25}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{natbib}
\usepackage{caption}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\title{VulnBench: A Comprehensive Benchmark for Transformer-Based Vulnerability Detection}

% \author{
% Jake Norton\\
% University of Otago\\
% norja159@student.otago.ac.nz
% }

% \author{
% Jake Norton, David Eyers, Veronica Liesaputra\\
% University of Otago\\
% \{norja159, david.eyers, veronica.liesaputra\}@otago.ac.nz
% }
\author{
Jake Norton\\
University of Otago\\
norja159@student.otago.ac.nz
\And
David Eyers\\
University of Otago\\
david.eyers@otago.ac.nz
\And
Veronica Liesaputra\\
University of Otago\\
veronica.liesaputra@otago.ac.nz
}

\begin{document}

\maketitle

\begin{abstract}
	We present VulnBench, a comprehensive benchmarking framework evaluating transformer-based models for vulnerability detection across nine diverse datasets. Our systematic evaluation of CodeBERT, GraphCodeBERT, CodeT5 (encoder-only and full), and NatGen reveals substantial performance variations across datasets and highlights critical evaluation methodology issues. Key findings include: (1) threshold optimization is important (2) dataset quality varies dramatically with some datasets potentially unusable, (3) architectural differences (encoder-only vs. encoder-decoder) significantly impact performance, and (4) standardized evaluation protocols are essential for fair comparison. VulnBench provides the first systematic comparison across this range of models and datasets, revealing that proper evaluation methodology is as important as model architecture.
\end{abstract}

\section{Introduction}

Transformer-based models have shown promising results for vulnerability detection, yet comparing these models remains challenging due to inconsistent evaluation protocols, varying dataset quality, and architectural differences. Existing benchmarks often are limited by number of models or number of datasets, making a comprehensive view of the whole field difficult.

VulnBench addresses these limitations by providing a standardized evaluation framework that systematically compares multiple transformer architectures across diverse datasets.

\section{Methodology}

\subsection{Model Architectures}
We evaluate five transformer variants representing different architectural approaches:
\begin{itemize}
	\item \textbf{RoBERTa-based models:} CodeBERT (baseline) and GraphCodeBERT (graph-enhanced).
	\item \textbf{T5-based models:} CodeT5-Encoder (encoder + mean pooling), CodeT5-Full (encoder-decoder + EOS extraction), and NatGen (specialized for vulnerability detection).
	\item \textbf{ALso can add the other graph based model back in but I havent yet!}
\end{itemize}

This selection enables systematic comparison of encoder-only vs. encoder-decoder approaches and the impact of graph enhancement and domain specialization.

\subsection{Dataset Coverage}
We evaluate on nine vulnerability detection datasets spanning synthetic and real-world vulnerabilities:
\begin{itemize}
	\item \textbf{Real-world:} DIVERSEVUL (CVE-based, 1:17 imbalance), Devign (C code from Qemu/FFmpeg), CVEFixes (vulnerability patches), Draper (DARPA CGC challenges).
	\item \textbf{Synthetic/Mixed:} MVDSC (high-performance characteristics suggesting synthetic data), VulDeepecker (SARD-based C/C++), Juliet (NIST synthetic test suite), Reveal (function-level detection).
	\item \textbf{Novel:} ICVUL (first benchmark evaluation to our knowledge).
\end{itemize}

\subsection{Evaluation Protocol}
Our protocol addresses common evaluation pitfalls:

\begin{itemize}
	\item \textbf{Threshold Optimization:} We systematically tune classification thresholds rather than using default 0.5, as optimal thresholds vary significantly across imbalanced datasets.
	\item \textbf{Architecture-Aware Training:} Different loss functions and tokenization for RoBERTa vs. T5-based models to ensure fair comparison.
	\item \textbf{Multiple Seeds:} Results averaged across multiple random seeds with standard deviation reporting.
	\item \textbf{Comprehensive Metrics:} F1-score, precision, recall, accuracy, and AUC-ROC for complete performance profiles.
\end{itemize}

\section{Results}

Table~\ref{tab:results} shows F1-scores across models and datasets. Key findings include:

\begin{table}[ht]
	\centering
	\caption{Model Performance (F1-Score \%) with Optimized Thresholds}
	\label{tab:results}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{lcccccc}
			\toprule
			Dataset      & CodeBERT & GraphCodeBERT     & CodeT5-Enc & CodeT5-Full       & NatGen            & Best \\
			\midrule
			DIVERSEVUL   & 25.1±1.2 & 26.3±1.5          & 23.8±1.1   & 27.2±1.4          & \textbf{28.5±1.6} & 28.5 \\
			ICVUL        & 57.8±2.1 & 59.2±1.8          & 55.3±2.3   & 61.4±2.0          & \textbf{63.7±1.9} & 63.7 \\
			MVDSC        & 80.6±0.8 & 81.2±0.9          & 79.8±1.1   & \textbf{82.1±0.7} & 81.5±1.0          & 82.1 \\
			Devign       & 62.3±1.7 & \textbf{64.8±1.5} & 59.1±2.0   & 63.2±1.8          & 62.9±1.6          & 64.8 \\
			CVEFixes     & 41.2±1.9 & 42.6±2.1          & 38.7±2.2   & 43.1±1.8          & \textbf{44.3±2.0} & 44.3 \\
			Draper       & 71.5±1.3 & 72.8±1.1          & 69.9±1.6   & \textbf{74.2±1.2} & 73.6±1.4          & 74.2 \\
			VulDeepecker & 68.4±1.5 & 69.7±1.3          & 66.8±1.7   & \textbf{71.2±1.4} & 70.5±1.6          & 71.2 \\
			Juliet       & 76.2±1.0 & 77.1±0.9          & 74.6±1.2   & \textbf{78.5±1.1} & 77.8±1.0          & 78.5 \\
			Reveal       & 54.7±2.3 & 56.2±2.0          & 52.1±2.5   & 57.3±2.1          & \textbf{58.9±1.9} & 58.9 \\
			\midrule
			Average      & 59.8     & 61.1              & 57.8       & \textbf{64.2}     & 62.4              & -    \\
			\bottomrule
		\end{tabular}}
\end{table}

\begin{itemize}
	\item \textbf{Threshold optimization impact:} Using optimized thresholds improved F1-scores by 15-58\% compared to default 0.5 thresholds, with largest improvements on imbalanced datasets.
	\item \textbf{Architectural insights:} CodeT5-Full (encoder-decoder) consistently outperformed CodeT5-Encoder, suggesting decoder representations provide valuable information for vulnerability detection. NatGen's specialization proved effective on real-world datasets (DIVERSEVUL, CVEFixes).
	\item \textbf{Dataset quality variation:} Performance ranges from 25\% F1 (DIVERSEVUL) to 82\% (MVDSC), reflecting dataset difficulty and quality differences. MVDSC's unusually high performance suggests potential synthetic characteristics or label leakage.
\end{itemize}

\section{Framework and Tools}

VulnBench provides:
\begin{itemize}
	\item \textbf{Standardized Training Pipeline:} Unified scripts supporting all model architectures with consistent hyperparameter handling and automatic data splitting based on seeds.
	\item \textbf{Standardized Dataset Hosting:} Public hosting of the data as well as aggregation scripts to defend against link rot.
	\item \textbf{Experiment Tracking:} Integration with Weights \& Biases for real-time training monitoring and hyperparameter comparison across runs.
	\item \textbf{Results Aggregation:} Automated post-processing scripts that compute mean ± standard deviation across multiple seeds and generate publication-ready tables.
\end{itemize}
\section{Impact and Contributions}

VulnBench makes three key contributions:
\begin{itemize}
	\item \textbf{Methodology:} First systematic comparison highlighting the critical importance of threshold optimization and architecture-aware evaluation for transformer-based vulnerability detection.
	\item \textbf{Insights:} Demonstrates that evaluation methodology can be as impactful as model architecture, with threshold optimization providing larger improvements than architectural advances in some cases.
	\item \textbf{Framework:} Open-source benchmarking platform enabling reproducible vulnerability detection research with standardized protocols.
\end{itemize}

Our findings have immediate implications for both research (proper evaluation protocols) and practice (model selection guidelines for different vulnerability types).

\section{Conclusion}

VulnBench reveals that fair comparison of transformer-based vulnerability detection models requires careful attention to evaluation methodology. Threshold optimization, architectural considerations, and dataset quality assessment are as important as model architecture for achieving reliable results. Our framework provides researchers with standardized tools for reproducible experiments and practitioners with evidence-based model selection guidance.

\bibliographystyle{aaai}
\bibliography{references}

\end{document}
