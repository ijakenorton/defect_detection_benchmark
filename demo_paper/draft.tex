\documentclass[letterpaper]{article}
\usepackage{aaai25}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{natbib}
\usepackage{caption}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\title{VulnBench: A Comprehensive Benchmark for Transformer-Based Vulnerability Detection}

\newcommand{\note}[2][red]{\textcolor{#1}{#2}}
\newcommand{\notedme}[1]{\note[blue]{[<Dave> #1]}}
%\newenvironment{scaffold}{\color{red}}{}
\newcommand{\change}[2][]{\textcolor{orange}{#2}}

%other cite option
% \author{
% Jake Norton, David Eyers, Veronica Liesaputra\\
% University of Otago\\
% \{jake.norton, david.eyers, veronica.liesaputra\}@otago.ac.nz
% }

\author{
Jake Norton\\
University of Otago\\
jake.norton@otago.ac.nz
\And
David Eyers\\
University of Otago\\
david.eyers@otago.ac.nz
\And
Veronica Liesaputra\\
University of Otago\\
veronica.liesaputra@otago.ac.nz
}

\begin{document}

\maketitle

\begin{abstract}
	Reproducible benchmarking of tools that automatically detect vulnerabilities in source code remains challenging due to inconsistent implementations, varying data preprocessing, and methodological flaws that compromise fair model comparison. In a recent study, 9 in 10 vulnerability detection studies were found to use inappropriate evaluation approaches, with models achieving high scores through spurious correlations rather than actual vulnerability detection. We present VulnBench, an extensible, open-source benchmarking tool that enables fair comparison across models and datasets. Our systematic evaluation of CodeBERT, GraphCodeBERT, CodeT5 (encoder-only and full), and NatGen across nine mostly C/C++ source code datasets reveals that proper threshold optimization can improve F1-scores by up to 54\%, as well as wide variation in F1-scores showing the large gap in the difficulty of the vulnerability dataset field. By standardising evaluation protocols, VulnBench enables researchers to distinguish between genuine model improvements and methodological artifacts as well as reducing wasteful duplication of effort spent on reproducing results.
\end{abstract}


\section{Introduction}

Software vulnerabilities represent security risks that can lead to critical data breaches, system compromises, and substantial economic losses. Traditional vulnerability detection methods rely heavily on static analysis tools and manual code reviews, which are resource-intensive and often impractical for detecting complex vulnerability patterns. The emergence of machine learning approaches, particularly deep learning models trained on large code corpora, has shown promise in automating vulnerability detection with improved accuracy and coverage due to the semantic patterns that these models encode~\citep{systematic2024survey}.

Recent advances in transformer-based models have revolutionized code understanding tasks. Models like CodeBERT \citep{feng2020codebert} and CodeT5 \citep{wang2021codet5} leverage pre-training on massive code repositories to capture semantic relationships in source code. Graph-enhanced approaches such as GraphCodeBERT \citep{guo2021graphcodebert} incorporate structural code information to improve understanding of data flow and control dependencies.

However, evaluating and comparing these models \change[faces significant methodological challenges]{is challenging}. \cite{risse2025top} found that vulnerability detection papers they studied often used flawed problem formulations, with models achieving high scores through misleading correlations rather than truly detecting vulnerabilities.

Furthermore, critical methodological considerations are often overlooked. Default classification thresholds (0.5) are frequently suboptimal for the severely imbalanced datasets common in vulnerability detection, yet systematic threshold optimization remains rare \citep{ghost2021, leevy2023optimal}. Recent work specifically examining vulnerability detection datasets confirms that proper threshold selection and loss function choice significantly impact performance on imbalanced data \citep{he2025imbalance}.

For the reasons stated above, VulnBench aims to provide researchers with a strong starting point to start their exploration into the space.

\section{VulnBench Architecture and Features}

VulnBench provides a comprehensive benchmarking platform addressing systematic evaluation challenges in vulnerability detection research. The framework implements four key architectural components:

\begin{itemize}
	\item \textbf{Unified Model Interface:} Provides consistent APIs over diverse transformer architectures. Researchers can easily add new models by implementing a standard interface, with built-in handling of architectural differences (encoder-only vs. encoder-decoder, different tokenization schemes and loss functions).

	\item \textbf{Automated Data Pipeline:} Handles nine vulnerability datasets preprocessed into a standard format. Features will include reproducible seed-based splitting, automatic dataset downloading, consistent function anonymization, and configurable train/validation/test ratios. Function anonymization is important for datasets like Juliet C/C++ \citep{juliet2022} as they include information about the code in the function declaration which tells the model how to classify it, e.g. `void CWE114\_Process\_Control\_bad()'. Custom datasets will require conversion to a given JSONL format, however, there are many script recipes provided to use as templates.

	\item \textbf{Evaluation Engine:} Implements threshold optimization, multi-seed statistical testing, and comprehensive metrics. Automatically generates performance reports with confidence intervals, statistical significance tests, and comparative analysis.

	\item \textbf{Experiment Tracking:} Integrated Weights \& Biases support with standalone logging fallback. Provides real-time training monitoring, hyperparameter comparison, and automated results aggregation across multiple seeds.
\end{itemize}

\section{Related Work}

\textbf{Code Understanding Benchmarks.} CodeXGLUE \citep{lu2021codexglue} represents the most comprehensive benchmarking effort for code intelligence, providing 14 datasets across 10 tasks including defect detection, with standardized evaluation protocols and baseline models. However, CodeXGLUE focuses broadly on code understanding rather than specifically addressing the methodological challenges in vulnerability detection evaluation.

\textbf{Vulnerability-Specific Benchmarks.} Many frameworks target vulnerability detection specifically. CASTLE \citep{castle2025} introduces a curated collection of 250 C programs for evaluating static analyzers, formal verification tools, and LLMs, with balanced vulnerable/non-vulnerable distributions.

\textbf{Critical Evaluation Issues.} Recent work has identified fundamental problems with current evaluation approaches \citep{risse2025top}. Studies of popular datasets (BigVul, CVEFixes, DiverseVul) reveal significant data leakage issues and labeling problems that compromise evaluation validity \citep{ullah2024vulnerability}.

\textbf{Benchmarking of Static Analysis Tools.} Web service vulnerability detection studies \citep{antunes2010benchmarking} propose benchmarking approaches for comparing penetration testing tools, static analyzers, and anomaly detectors using metrics like precision, recall, and F-measure. Comprehensive evaluations of memory error detectors \citep{zhang2021evaluating} against benchmark datasets reveal varied accuracy across vulnerability categories and highlight the challenges of making fair comparisons.

\textbf{Dataset Quality and Standardization.} Systematic surveys \citep{systematic2024survey} reveal that 39.1\% of vulnerability detection studies use hybrid data sources combining benchmarks, repositories, and projects, while 37.6\% rely on standardized benchmark datasets. However, many proposed benchmarking frameworks \citep{lin2020deep} focus on specific neural network architectures rather than providing systematic evaluation methodology.

\textbf{Distinguishing Characteristics of VulnBench.} Unlike existing work, VulnBench addresses the methodological issues by providing: (1) standardized threshold optimization addressing severe class imbalances, (2) architecture-aware evaluation protocols handling transformer model differences, and (3) systematic dataset quality assessment revealing artifacts that compromise research validity. Existing frameworks serve different purposes: CodeXGLUE provides broad benchmarks for general code understanding tasks, while OWASP Benchmark and CASTLE focus on evaluating specific security tools in controlled environments.

\section{Demonstration Results}

\begin{table}[ht]
	\centering
	\caption{Best F1-Score Model per Dataset, using a random 80/10/10 train–validation–test split with three unique seeds.}
	\label{tab:results}
	\begin{tabular}{lcc}
		\toprule
		Dataset      & Best Model    & F1-Score    \\
		\midrule
		CVEFixes     & GraphCodeBERT & 0.603±0.006 \\
		Devign       & CodeT5        & 0.671±0.010 \\
		DiverseVul   & NatGen        & 0.307±0.025 \\
		Draper       & CodeT5        & 0.609±0.007 \\
		ICVul        & NatGen        & 0.586±0.003 \\
		Juliet       & NatGen        & 0.900±0.003 \\
		Reveal       & GraphCodeBERT & 0.486±0.007 \\
		VulDeepecker & CodeT5        & 0.959±0.001 \\
		\bottomrule
	\end{tabular}
\end{table}

VulnBench evaluation across four transformer architectures and eight datasets reveals critical methodological insights:
\begin{enumerate}
	\item \textbf{Threshold optimization universally improves performance}---100\% of model-dataset combinations show positive F1 gains (median: +0.082, best: +0.542)
	\item \textbf{Dataset quality varies dramatically}---synthetic datasets (Juliet, VulDeepecker) achieve F1 > 0.9 while real-world datasets struggle (DiverseVul: 0.307)
	\item \textbf{Architecture matters}---CodeT5 and NatGen consistently outperform encoder-only models.
\end{enumerate}

Complete results and reproducible code available online\footnote{\url{https://github.com/ijakenorton/defect_detection_benchmark}}.

\textbf{Practical Impact:} VulnBench enables researchers to distinguish between genuine model improvements and methodological artifacts. Our evaluation confirms recent findings about dataset quality issues~\citep{ding2025vulnerability,chen2023diversevul}. Synthetic datasets in our evaluation (Juliet: F1=0.900, VulDeepecker: F1=0.959) show artificially inflated performance compared to real-world datasets (DiverseVul: F1=0.307, CVEFixes: F1=0.603), consistent with reports of significant data quality problems including poor label accuracy and high duplication rates~\cite{ding2025vulnerability}. While VulnBench does not resolve underlying dataset quality problems---which persist across widely-used benchmarks---it provides transparent evaluation protocols that expose these artifacts rather than obscure them.

\section{Availability and Future Extensions}

VulnBench is available as an open-source framework. Future development includes: support for additional transformer architectures; other state-of-the-art models like LineVul\citep{linevul2022}; integration of more datasets e.g. PrimeVul\citep{ding2025vulnerability}.

\section*{Acknowledgements}
This research was supported by the Ministry of Business, Innovation and Employment (MBIE), New Zealand. The author gratefully acknowledges this funding, which made this work possible.
We also acknowledge the open-source community for providing the datasets and tools that enabled this evaluation framework.

\bibliographystyle{aaai}
\bibliography{references}

\end{document}
