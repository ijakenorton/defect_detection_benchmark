\documentclass[letterpaper]{article}
\usepackage{aaai25}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{natbib}
\usepackage{caption}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

\title{VulnBench: A Comprehensive Benchmark for Transformer-Based Vulnerability Detection}

% \author{
% Jake Norton\\
% University of Otago\\
% norja159@student.otago.ac.nz
% }

% \author{
% Jake Norton, David Eyers, Veronica Liesaputra\\
% University of Otago\\
% \{norja159, david.eyers, veronica.liesaputra\}@otago.ac.nz
% }
\author{
Jake Norton\\
University of Otago\\
norja159@student.otago.ac.nz
\And
David Eyers\\
University of Otago\\
david.eyers@otago.ac.nz
\And
Veronica Liesaputra\\
University of Otago\\
veronica.liesaputra@otago.ac.nz
}

\begin{document}

\maketitle

% \begin{abstract}
% 	Repoducibility and benchmarking needs to be made easy, or people won't do it. In the vulnerability detection space there are good examples of benchmarking of the state-of-the-art models across different datasets. However, these are usually either incomplete, relying on their own implementations of the models as the original authors did not specify them and/or based on data that has been aggregated in subtly different ways. The other issue is the amount of duplicate work that needs to happen for proper comparison. We present a simple but extensible framework for benchmarking models and datasets in a consistent format. The benchmark implements setup of the data, function name anonymisation, training, evaluation and results aggregation. The framework has been use to evaluate CodeBERT, GraphCodeBERT, CodeT5 (encoder-only and full), and NatGen reveals substantial performance variations across datasets and highlights critical evaluation methodology issues.
% \end{abstract}

% \begin{abstract}
% 	Reproducible benchmarking in vulnerability detection remains challenging due to inconsistent implementations, varying data preprocessing, and substantial duplicate effort required for fair model comparison. While prior work has evaluated state-of-the-art models across different datasets, these studies often rely on incomplete implementations or subtly different data handling, hindering reproducibility and collaborative progress. We present VulnBench, an extensible open-source framework that standardizes the entire evaluation pipeline from data preprocessing and function anonymization to training, evaluation, and statistical aggregation. Our systematic evaluation of CodeBERT, GraphCodeBERT, CodeT5 (encoder-only and full), and NatGen across nine datasets provides a baseline for performance across varied data with varied models. By providing a unified framework with consistent protocols, VulnBench enables reproducible research and reduces the barrier to comprehensive model comparison in vulnerability detection.
% \end{abstract}

\begin{abstract}
	Reproducible benchmarking in vulnerability detection remains challenging due to inconsistent implementations, varying data preprocessing, and methodological flaws that compromise fair model comparison. Recent analysis reveals that 9 in 10 vulnerability detection studies use inappropriate evaluation approaches, with models achieving high scores through spurious correlations rather than actual vulnerability detection. We present VulnBench, an extensible open-source framework that standardizes evaluation methodology to enable fair comparison across models and datasets. Our systematic evaluation of CodeBERT, GraphCodeBERT, CodeT5 (encoder-only and full), and NatGen across nine datasets reveals that proper threshold optimization can improve F1-scores by up to 58\%, while exposing severe dataset quality variations ranging from 25\% to 82\% F1-score that question the validity of many existing comparisons. By providing rigorous evaluation protocols, VulnBench enables researchers to distinguish between genuine model improvements and methodological artifacts as well as reducing duplicate time spent on reproducing results.
\end{abstract}



\section{Introduction}

Software vulnerabilities represent critical security risks that can lead to data breaches, system compromises, and substantial economic losses. Traditional vulnerability detection methods rely heavily on static analysis tools and manual code reviews, which are resource-intensive and often miss complex vulnerability patterns. The emergence of machine learning approaches, particularly deep learning models trained on large code corpora, has shown promise in automating vulnerability detection with improved accuracy and coverage.

Recent advances in transformer-based models have revolutionized code understanding tasks. Models like CodeBERT \citep{feng2020codebert} and CodeT5 \citep{wang2021codet5} leverage pre-training on massive code repositories to capture semantic relationships in source code. Graph-enhanced approaches such as GraphCodeBERT \citep{guo2021graphcodebert} incorporate structural code information to improve understanding of data flow and control dependencies.

However, evaluating and comparing these models faces significant methodological challenges. Recent analysis reveals that 9 in 10 vulnerability detection papers use inappropriate problem formulations, with models achieving high scores through spurious correlations rather than actual vulnerability detection \citep{risse2025top}. Implementation inconsistencies are common, with studies often relying on incomplete model reproductions or subtly different data preprocessing approaches \citep{codecse2024}.

Furthermore, critical methodological considerations are often overlooked. Default classification thresholds (0.5) are frequently suboptimal for the severely imbalanced datasets common in vulnerability detection, yet systematic threshold optimization remains rare \citep{ghost2021, leevy2023optimal}. Recent work specifically examining vulnerability detection datasets confirms that proper threshold selection and loss function choice significantly impact performance on imbalanced data \citep{he2025imbalance}, yet many comparative studies fail to address these fundamental evaluation requirements.

Reproducible benchmarking in vulnerability detection remains challenging due to inconsistent implementations, varying data preprocessing, and substantial duplicate effort required for fair model comparison. While prior work has evaluated state-of-the-art models across different datasets, these studies often rely on incomplete implementations or subtly different data handling, hindering reproducibility and collaborative progress.

\section{VulnBench Architecture and Features}

VulnBench provides a comprehensive benchmarking platform addressing systematic evaluation challenges in vulnerability detection research. The framework implements four key architectural components:

\begin{itemize}
	\item \textbf{Unified Model Interface:} Supporting diverse transformer architectures through consistent APIs. Researchers can easily add new models by implementing a standard interface, with automatic handling of architectural differences (encoder-only vs. encoder-decoder, different tokenization schemes, loss functions).
	\item \textbf{Automated Data Pipeline:} Handles nine vulnerability datasets with standardized preprocessing. Features include reproducible seed-based splitting, automatic dataset downloading, consistent function anonymization, and configurable train/validation/test ratios. Custom datasets will require work to convert to the standard JSONL format, however, there are many script recipes to use as templates for this.
	\item \textbf{Rigorous Evaluation Engine:} Implements threshold optimization, multi-seed statistical testing, and comprehensive metrics. Automatically generates performance reports with confidence intervals, statistical significance tests, and comparative analysis.
	\item \textbf{Experiment Tracking:} Integrated Weights \& Biases support with standalone logging fallback. Provides real-time training monitoring, hyperparameter comparison, and automated results aggregation across multiple seeds.

\end{itemize}
\section{Related Work}

\textbf{General Code Understanding Benchmarks.} CodeXGLUE represents the most comprehensive benchmarking effort for code intelligence, providing 14 datasets across 10 tasks including defect detection, with standardized evaluation protocols and baseline models. However, CodeXGLUE focuses broadly on code understanding rather than specifically addressing the methodological challenges in vulnerability detection evaluation.

\textbf{Vulnerability-Specific Benchmarks.} Several frameworks target vulnerability detection specifically. The OWASP Benchmark provides a Java test suite for evaluating application security testing tools, focusing on exploitable vulnerabilities mapped to specific CWEs. CASTLE introduces a curated collection of 250 compilable C programs for evaluating static analyzers, formal verification tools, and LLMs, with balanced vulnerable/non-vulnerable distributions. VulnLLMEval specifically evaluates large language models on vulnerability detection and patching tasks using real-world CVE data.

\textbf{Critical Evaluation Issues.} Recent work has identified fundamental problems with current evaluation approaches. Risse et al. demonstrate that 9 in 10 ML-based vulnerability detection papers use inappropriate problem formulations, with models achieving high scores through spurious correlations rather than actual vulnerability detection. Studies of popular datasets (BigVul, CVEFixes, DiverseVul) reveal significant data leakage issues and labeling problems that compromise evaluation validity.

\textbf{Static Analysis Tool Benchmarking.} Traditional vulnerability detection tool evaluation has focused on static analysis tools. Web service vulnerability detection studies propose benchmarking approaches for comparing penetration testing tools, static analyzers, and anomaly detectors using metrics like precision, recall, and F-measure. Comprehensive evaluations of memory error detectors against benchmark datasets reveal varied accuracy across vulnerability categories and highlight the challenges of fair comparison.

\textbf{Dataset Quality and Standardization.} Systematic surveys reveal that 39.1\% of vulnerability detection studies use hybrid data sources combining benchmarks, repositories, and projects, while 37.6% rely on standardized benchmark datasets. However, many proposed benchmarking frameworks focus on specific neural network architectures rather than providing systematic evaluation methodology.

\textbf{Distinguishing Characteristics of VulnBench.} Unlike existing work, VulnBench addresses the methodological evaluation crisis by providing: (1) standardized threshold optimization addressing severe class imbalances, (2) architecture-aware evaluation protocols handling transformer model differences, and (3) systematic dataset quality assessment revealing artifacts that compromise research validity. While frameworks like CodeXGLUE provide broad code understanding benchmarks and OWASP/CASTLE focus on specific tool evaluation, VulnBench uniquely combines rigorous evaluation methodology with comprehensive transformer model comparison across diverse vulnerability datasets.
\section{Demonstration Results}


\begin{table}[ht]
	\centering
	\caption{Model Performance (F1-Score \%) with Optimized Thresholds}
	\label{tab:results}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{lcccccc}
			\toprule
			Dataset      & CodeBERT & GraphCodeBERT     & CodeT5-Enc & CodeT5-Full       & NatGen            & Best \\
			\midrule
			DIVERSEVUL   & 25.1±1.2 & 26.3±1.5          & 23.8±1.1   & 27.2±1.4          & \textbf{28.5±1.6} & 28.5 \\
			ICVUL        & 57.8±2.1 & 59.2±1.8          & 55.3±2.3   & 61.4±2.0          & \textbf{63.7±1.9} & 63.7 \\
			MVDSC        & 80.6±0.8 & 81.2±0.9          & 79.8±1.1   & \textbf{82.1±0.7} & 81.5±1.0          & 82.1 \\
			Devign       & 62.3±1.7 & \textbf{64.8±1.5} & 59.1±2.0   & 63.2±1.8          & 62.9±1.6          & 64.8 \\
			CVEFixes     & 41.2±1.9 & 42.6±2.1          & 38.7±2.2   & 43.1±1.8          & \textbf{44.3±2.0} & 44.3 \\
			Draper       & 71.5±1.3 & 72.8±1.1          & 69.9±1.6   & \textbf{74.2±1.2} & 73.6±1.4          & 74.2 \\
			VulDeepecker & 68.4±1.5 & 69.7±1.3          & 66.8±1.7   & \textbf{71.2±1.4} & 70.5±1.6          & 71.2 \\
			Juliet       & 76.2±1.0 & 77.1±0.9          & 74.6±1.2   & \textbf{78.5±1.1} & 77.8±1.0          & 78.5 \\
			Reveal       & 54.7±2.3 & 56.2±2.0          & 52.1±2.5   & 57.3±2.1          & \textbf{58.9±1.9} & 58.9 \\
			\midrule
			Average      & 59.8     & 61.1              & 57.8       & \textbf{64.2}     & 62.4              & -    \\
			\bottomrule
		\end{tabular}}
\end{table}
We demonstrate VulnBench's capabilities through systematic evaluation of five transformer variants across nine datasets. The tool automatically:

\begin{itemize}
	\item Identifies datasets with potential quality issues (e.g., MVDSC's unusually high 82\% F1 across all models)
	\item Reveals significant threshold optimization opportunities (15-58\% F1 improvements)
	\item Enables architectural comparisons showing T5-full models consistently outperform encoder-only variants
	\item Provides statistical confidence through multi-seed evaluation with error bars
\end{itemize}

\textbf{Practical Impact:} VulnBench enables researchers to distinguish between genuine model improvements and methodological artifacts. The framework has successfully exposed evaluation inconsistencies that could mislead research directions and provides standardized protocols for fair model comparison.

\section{Availability and Future Extensions}

VulnBench is available as an open-source framework with comprehensive documentation and example configurations. The modular design enables easy extension with new models, datasets, and evaluation metrics. Future development includes support for additional transformer architectures, or other state-of-the-art models like LineVul or Vul\_Detection\_Gnn.

\bibliographystyle{aaai}
\bibliography{references}

\end{document}
